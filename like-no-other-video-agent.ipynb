{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53f09fcd-c60f-4ce7-8fc2-416931204400",
   "metadata": {},
   "source": [
    "# \"Like No Other\" Video Agent\n",
    "\n",
    "The \"Like No Other\" video agent is designed to recreate your favourite ad. I gave it tools and let it be the Creative Director, Producer and Post Production Producer of the videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb65f036-55cd-4660-88ed-eeaf9255a732",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai-agents nest_asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b1c5e8-d6f3-434f-9e2a-18afcf2fff9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install google-adk\n",
    "%pip install google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e79a804-8bec-4c77-a63c-10c5f05b4ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install fal-client ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d08603-d9b5-40a7-934a-780cdf71cfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fcc0f794-ceac-4c18-9760-d7dffb29d2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "import asyncio\n",
    "import fal_client\n",
    "import base64\n",
    "import subprocess\n",
    "\n",
    "from agents import Agent as OpenAIAgent, Runner as OpenAIRunner, function_tool, WebSearchTool\n",
    "from agents.run_context import RunContextWrapper\n",
    "\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from typing import Any, Dict, Union\n",
    "from google.adk.tools import LongRunningFunctionTool\n",
    "from google.adk.agents import Agent as ADKAgent, SequentialAgent\n",
    "from google.genai.types import GenerationConfig\n",
    "from google.adk.runners import Runner as ADKRunner\n",
    "from google.adk.sessions import InMemorySessionService\n",
    "from google.genai.types import Content, Part\n",
    "from google.adk.tools import google_search\n",
    "from google.adk.tools.tool_context import ToolContext\n",
    "from google.adk.models.lite_llm import LiteLlm\n",
    "from google.adk.planners import BuiltInPlanner\n",
    "\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from fal_client import upload_file, subscribe, InProgress, submit, result\n",
    "\n",
    "from pathlib import Path\n",
    "from IPython.display import Audio as IPAudio\n",
    "from IPython.display import display, Video, Image, HTML\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca119c5b-aed8-414f-8702-898fe583658d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API keys loaded!\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "FAL_KEY = os.getenv(\"FAL_KEY\")\n",
    "os.environ[\"FAL_KEY\"] = FAL_KEY\n",
    "\n",
    "PERPLEXITY_SONAR_API_KEY=os.getenv(\"PERPLEXITY_SONAR_API_KEY\")\n",
    "GEMINI_API_KEY=os.getenv(\"GOOGLE_API_KEY\")\n",
    "OPENAI_API_KEY=os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "\n",
    "if GEMINI_API_KEY and OPENAI_API_KEY and PERPLEXITY_SONAR_API_KEY:\n",
    "    print(\"API keys loaded!\")\n",
    "else:\n",
    "    print(\"One or more API keys are not loaded!\")\n",
    "\n",
    "llm = LiteLlm(\n",
    "    model=\"openai/gpt-4o\",\n",
    "    temperature=0,\n",
    "    num_retries=0,\n",
    "    timeout=60,\n",
    ")\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_API_KEY, max_retries=1, timeout=6000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aafec6ed-adf8-4d0e-aaef-549974b2e171",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import Agent, Runner, function_tool, WebSearchTool\n",
    "from agents.run_context import RunContextWrapper\n",
    "\n",
    "@function_tool\n",
    "async def write_to_markdown(ctx: RunContextWrapper, content: str, filename: str = None) -> str:\n",
    "    \"\"\"Writes the given content to a markdown file, always named query_reply.md.\"\"\"\n",
    "    with open(\"query_reply.md\", \"w\") as f:\n",
    "        f.write(content)\n",
    "    return \"Data written to query_reply.md\"\n",
    "\n",
    "\n",
    "# Create the agent\n",
    "openai_agent = OpenAIAgent(\n",
    "    name=\"WebSearchAgent\",\n",
    "    instructions=\"\"\"\n",
    "        You are a senior creative strategist and advertising researcher. Your job is to analyze ad campaigns by searching the web and structuring your findings into a rich, human-readable report that will guide an AI in recreating the ad experience.\n",
    "        \n",
    "        Given a user query about a specific ad campaign, follow these steps:\n",
    "        \n",
    "        1. Search across multiple sources (articles, forums, ad reviews, Wikipedia, YouTube comments, award sites).\n",
    "        2. Synthesize the campaign information with vivid detail.\n",
    "        3. Structure the findings using the following **Markdown template.** Every section must be included, even if you only have partial data.\n",
    "        5. Save the full result using the write_to_markdown tool as query_reply.md.\n",
    "\n",
    "        **Markdown template:**\n",
    "        ---\n",
    "        \n",
    "        # The Ad in a Nutshell\n",
    "        \n",
    "        (A brief 3–5 sentence summary of the ad’s core idea, product, visual hook, and when/where it aired. Include at least two source links.)\n",
    "        \n",
    "        Example:  \n",
    "        It’s a ~1-minute spot from Coca-Cola Life, originally aired in Argentina around 2013 [source](https://en.wikipedia.org).  \n",
    "        Opens with a couple joyfully discovering they're expecting, followed by a montage of real-life parenthood: sleepless nights, messy meals, toys scattered everywhere — beautifully honest and relatable [source](https://churchpop.com).  \n",
    "        The dad takes a swig of Coke Life, then his wife shows another positive pregnancy test — he looks shocked, then smiles. It ends with them embracing, toddler in tow.\n",
    "        \n",
    "        # The Scenes\n",
    "        \n",
    "        (A visually rich and **very detailed** breakdown of what happens — in order — including setting, props, characters, framing, colors, and motion. Aim for 3–5 bullet points. Link where possible.)\n",
    "        \n",
    "        - A cheerful outdoor cook-out with kids running around, corn on the cob, pizza, and Coca-Cola bottles in a big ice bucket.  \n",
    "        - Diverse families chatting, smiling, and toasting — showing that Coke brings everyone together.  \n",
    "        - Subtle product placement of Sprite, Honest juice, and Coke Zero Sugar, hinting at a portfolio message [source](https://marketingdive.com).\n",
    "        \n",
    "        # Why It Resonated\n",
    "        \n",
    "        (Explain why the ad worked — emotionally, culturally, psychologically. Include audience reactions, critiques, and quotes. 3–5 bullet points is ideal.)\n",
    "        \n",
    "        - **Authenticity over cutesiness**: It doesn’t sugarcoat parenting. It shows real mess, exhaustion, and love [source](https://fastcompany.com).  \n",
    "        - **Emotional comedy**: It's “right down to the uglier parenting realities” yet makes you “smile and weep” [source](https://fastcompany.com).  \n",
    "        - **Deep relatability**: Viewers said “If you're a parent, this will hit you hard” [source](https://x.com).\n",
    "        \n",
    "        # The Music\n",
    "        \n",
    "        (If applicable, name the artist, song title, and year. Describe how the music supports the tone. If no music, describe use of ambient audio.)\n",
    "        \n",
    "        The soundtrack was “Hold My Hand” by English singer Jess Glynne (2015).  \n",
    "        Its upbeat, house-pop rhythm underscored the warmth and unity of the family moment [source](https://etsy.com), [source](https://bustle.com).\n",
    "        \n",
    "        ---\n",
    "\n",
    "        \"\"\",\n",
    "    tools=[\n",
    "        WebSearchTool(),\n",
    "        write_to_markdown,\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b1a23252-929e-48fc-bd0e-cd553780269d",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def ask_openai_agent(tool_context: ToolContext) -> str:\n",
    "    \"\"\"\n",
    "    Calls your existing OpenAI agent using query from tool_context\n",
    "\n",
    "    Args:\n",
    "        tool_context (ToolContext): The user query passed over the tool_context\n",
    "    \n",
    "    \"\"\"\n",
    "    query = tool_context.state.get(\"user_query\")\n",
    "    \n",
    "    if not query:\n",
    "        return \"No user query found in tool context state\"\n",
    "    \n",
    "    result = await OpenAIRunner.run(openai_agent, query)\n",
    "\n",
    "    print(result.final_output)\n",
    "    \n",
    "    return result.final_output\n",
    "\n",
    "\n",
    "async def read_markdown() -> str:\n",
    "    \"\"\"\n",
    "    Enables you to read the query_reply.md\n",
    "    \"\"\"\n",
    "    file_path = \"query_reply.md\"\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            return f.read()\n",
    "    except FileNotFoundError:\n",
    "        return \"❌ query_reply.md file not found.\"\n",
    "    except Exception as e:\n",
    "        return f\"❌ Error reading file: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3d03df53-dbe0-4bd7-88a9-e586032cbc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_image_imagen4(tool_context: ToolContext, prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates 1 image using Google's Imagen 4 model based on the provided prompt.\n",
    "    Saves the image locally and stores the prompt and image path in the tool context.\n",
    "    \n",
    "    Args:\n",
    "        prompt (str): The text prompt for image generation\n",
    "    \n",
    "    Note: Imagen 4 is a text-to-image model, not image-to-image like Flux Pro.\n",
    "    \n",
    "    \"\"\"\n",
    "    campaign_data = tool_context.state.get(\"campaign_data\", {})\n",
    "    \n",
    "    print(\"🎨 Using Google's Imagen 4 for image generation\\n\")\n",
    "\n",
    "    if prompt:\n",
    "            print(prompt)\n",
    "        \n",
    "    try:\n",
    "        print(f\"\\n🖼️ Generating image with Imagen 4:\\n{prompt}\\n\")\n",
    "        output_filename = f\"imagen4_output.png\"\n",
    "        output_path = os.path.join(os.getcwd(), output_filename)\n",
    "        \n",
    "        # Submit generation request to Imagen 4\n",
    "        handler = fal_client.submit(\n",
    "            \"fal-ai/imagen4/preview\",\n",
    "            arguments={\n",
    "                \"prompt\": prompt,\n",
    "                \"negative_prompt\": \"blurry, low quality, distorted, watermark, text overlay, poor lighting, oversaturated, undersaturated, grainy, artifacts\",\n",
    "                \"aspect_ratio\": \"16:9\",  # Options: \"1:1\", \"16:9\", \"9:16\", \"3:4\", \"4:3\"\n",
    "                \"num_images\": 1,\n",
    "                \"seed\": 123456  # For reproducible generation\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Get result\n",
    "        result_data = fal_client.result(\"fal-ai/imagen4/preview\", handler.request_id)\n",
    "        image_info = result_data[\"images\"][0]\n",
    "        generated_url = image_info.get(\"url\")\n",
    "        \n",
    "        if not generated_url:\n",
    "            return f\"❌ No image URL returned for the image.\"\n",
    "        \n",
    "        # Download the image content\n",
    "        if generated_url.startswith(\"data:image\"):\n",
    "            _, encoded = generated_url.split(\",\", 1)\n",
    "            image_bytes = base64.b64decode(encoded)\n",
    "        else:\n",
    "            response = requests.get(generated_url)\n",
    "            response.raise_for_status()\n",
    "            image_bytes = response.content\n",
    "        \n",
    "        with open(output_path, \"wb\") as f:\n",
    "            f.write(image_bytes)\n",
    "        \n",
    "        # Display in notebook\n",
    "        try:\n",
    "            display(Image(output_path))\n",
    "        except Exception:\n",
    "            print(\"⚠️ Could not display image in notebook.\")\n",
    "        \n",
    "        # Save prompt and image path in context\n",
    "        tool_context.state[\"image_prompt\"] = prompt\n",
    "        tool_context.state[\"image_path\"] = output_path\n",
    "        \n",
    "        print(f\"✅ Successfully generated and saved image using Google's Imagen 4. Image saved to: {output_path}\")\n",
    "        \n",
    "        return f\"✅ Successfully generated and saved image using Google's Imagen 4. Image saved to: {output_path}\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"❌ Error during Imagen 4 generation: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2c8b3342-ffb7-4788-8b76-c2a3cf2b97e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_image_gpt_image_1(tool_context: ToolContext, prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates an image using OpenAI's gpt-image-1 and saves it locally.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The text prompt for image generation\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    if not prompt:\n",
    "        return \"Error: No image prompt provided.\"\n",
    "\n",
    "    try:\n",
    "        # Generate the image\n",
    "        response = client.images.generate(\n",
    "            model=\"gpt-image-1\",\n",
    "            prompt=prompt,\n",
    "            size=\"1536x1024\", #Possible enum values: auto, 1024x1024, 1536x1024, 1024x1536\n",
    "            quality=\"high\",\n",
    "            n=1\n",
    "        )\n",
    "\n",
    "        # Extract base64 image data\n",
    "        image_b64 = response.data[0].b64_json\n",
    "        image_bytes = base64.b64decode(image_b64)\n",
    "\n",
    "        # Save to file\n",
    "        image_path = os.path.join(os.getcwd(), \"generated_image.png\")\n",
    "        with open(image_path, 'wb') as f:\n",
    "            f.write(image_bytes)\n",
    "\n",
    "        # Display in notebook\n",
    "        try:\n",
    "            display(Image(image_path))\n",
    "        except:\n",
    "            print(\"Note: Could not display image in notebook\")\n",
    "        \n",
    "\n",
    "        tool_context.state[\"image_prompt\"] = prompt\n",
    "        tool_context.state[\"image_path\"] = image_path\n",
    "        \n",
    "        print(f\"✅ Successfully generated and saved image using OpenAI's gpt-image-1. Image saved to: {image_path}\")\n",
    "\n",
    "        return f\"Successfully generated and saved image using OpenAI's gpt-image-1. Image saved to: {image_path}\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Error generating image: {e}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c7184d19-0ce3-4f85-beac-c9f6b109d23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_image_flux_pro(tool_context: ToolContext, prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates an image using Flux Pro's text-to-image model and saves it locally.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The text prompt for image generation\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    if not image_prompt:\n",
    "        return \"Error: No image prompt provided.\"\n",
    "\n",
    "    print(f\"\\n🎨 Generating image with Flux Pro:\\n{prompt}\\n\")\n",
    "\n",
    "    try:\n",
    "        handler = submit(\n",
    "            \"fal-ai/flux-pro/kontext/text-to-image\",\n",
    "            arguments={\n",
    "                \"prompt\": prompt,\n",
    "                \"guidance_scale\": 3.5,\n",
    "                \"num_images\": 1,\n",
    "                \"safety_tolerance\": \"2\",\n",
    "                \"output_format\": \"png\",\n",
    "                \"aspect_ratio\": \"16:9\", #Possible enum values: 21:9, 16:9, 4:3, 3:2, 1:1, 2:3, 3:4, 9:16, 9:21\n",
    "                \"sync_mode\": True\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Wait for the result and get image URL\n",
    "        result_data = result(\"fal-ai/flux-pro/kontext/text-to-image\", handler.request_id)\n",
    "        image_url = result_data[\"images\"][0][\"url\"]\n",
    "\n",
    "        # try:\n",
    "        #     print(image_url)\n",
    "        # except:\n",
    "        #     print(\"Image URL is in unexpected format!\")\n",
    "\n",
    "        # Decode base64 image from data URI\n",
    "        if image_url.startswith(\"data:image\"):\n",
    "            header, encoded = image_url.split(\",\", 1)\n",
    "            image_bytes = base64.b64decode(encoded)\n",
    "        else:\n",
    "            # fallback in case it returns a real URL (not likely for this model)\n",
    "            import requests\n",
    "            image_bytes = requests.get(image_url).content\n",
    "\n",
    "        # Save image to file\n",
    "        image_path = os.path.join(os.getcwd(), \"flux_pro_generated_image.png\")\n",
    "        with open(image_path, \"wb\") as f:\n",
    "            f.write(image_bytes)\n",
    "\n",
    "        # Display in notebook\n",
    "        try:\n",
    "            display(Image(image_path))   \n",
    "        except:\n",
    "            print(\"Note: Could not display image in notebook\")\n",
    "\n",
    "        tool_context.state[\"image_prompt\"] = prompt\n",
    "        tool_context.state[\"image_path\"] = image_path\n",
    "        \n",
    "        print(f\"✅ Successfully generated and saved image using Flux Pro's kontext/text-to-image. Image saved to: {image_path}\")\n",
    "\n",
    "        return f\"uccessfully generated and saved image using Flux Pro's kontext/text-to-image. Image saved to: {image_path}\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"❌ Error generating image with Flux Pro: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "46ad6a6f-003c-4aa4-99a6-dac101023586",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_video_player(video_path):\n",
    "    display(HTML(\"<h3 style='text-align: left;'>📹 Generated Video</h3>\"))\n",
    "\n",
    "    display(Video(video_path, width=600, embed=True))\n",
    "\n",
    "    display(HTML(f\"<p style='text-align: left;'><strong>Video Path:</strong> {video_path}</p>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4a0cc69d-a789-44af-b423-f5c5027f2115",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_video_veo3(tool_context: ToolContext, video_prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates 1 video using Google's Veo3 text-to-video model. Video length is 8 seconds!\n",
    "    Stores the resulting video path in the tool_context.\n",
    "    \n",
    "    Args:\n",
    "        video_prompt (str): Text prompt for video generation\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if video_prompt:\n",
    "            print(video_prompt)\n",
    "        \n",
    "        def on_queue_update(update):\n",
    "            if isinstance(update, InProgress):\n",
    "                for log in update.logs:\n",
    "                    print(log[\"message\"])\n",
    "        \n",
    "        print(f\"🚀 Submitting video request to Veo3...\")\n",
    "        result = subscribe(\n",
    "            \"fal-ai/veo3\",\n",
    "            arguments={\n",
    "                \"prompt\": video_prompt,\n",
    "                \"aspect_ratio\": \"16:9\",\n",
    "                \"duration\": \"8s\",\n",
    "                \"negative_prompt\": \"blurry, distorted, unrealistic hands, warped faces, glitchy movement, pixelated, low quality, jerky camera, unmentioned characters, unnatural lighting\",\n",
    "                \"enhance_prompt\": True,\n",
    "                \"generate_audio\": False,\n",
    "            },\n",
    "            with_logs=True,\n",
    "            on_queue_update=on_queue_update,\n",
    "        )\n",
    "        \n",
    "        video_url = result.get(\"video\", {}).get(\"url\")\n",
    "        if not video_url:\n",
    "            return f\"❌ No video URL returned.\"\n",
    "        \n",
    "        response = requests.get(video_url)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "        video_path = os.path.join(os.getcwd(), f\"generated_video_{timestamp}.mp4\")\n",
    "        \n",
    "        with open(video_path, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "\n",
    "        try:\n",
    "            display(Video(video_path, width=600, embed=True))\n",
    "        except Exception:\n",
    "            print(f\"⚠️ Could not display video in notebook.\")\n",
    "\n",
    "        # Create unique key based on timestamp\n",
    "        unique_key = f\"video_path_{timestamp}\"\n",
    "        tool_context.state[unique_key] = video_path\n",
    "        \n",
    "        # Also maintain a list of all video paths for easy access\n",
    "        if \"all_video_paths\" not in tool_context.state:\n",
    "            tool_context.state[\"all_video_paths\"] = []\n",
    "        tool_context.state[\"all_video_paths\"].append(video_path)\n",
    "        \n",
    "        print(f\"✅ Successfully generated and saved video from Veo3\") \n",
    "        print(f\"📁 Video saved to: {video_path}\")\n",
    "        print(f\"🔑 Context key: {unique_key}\")\n",
    "        \n",
    "        return f\"✅ Successfully generated and saved video from Veo3. Video saved to: {video_path} (key: {unique_key})\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"❌ Error during Veo3 video generation: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "02d7ec3a-a4ee-48fa-a832-a8a17b599cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_video_seedance_pro(tool_context: ToolContext, video_prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates 1 video using Bytedance's Seedance 1.0 Pro text-to-video model. Video length is 10 seconds!\n",
    "    Stores the resulting video path in the tool_context.\n",
    "    \n",
    "    Args:\n",
    "        video_prompt (str): Text prompt for video generation\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if video_prompt:\n",
    "            print(video_prompt)\n",
    "        \n",
    "        def on_queue_update(update):\n",
    "            if isinstance(update, InProgress):\n",
    "                for log in update.logs:\n",
    "                    print(log[\"message\"])\n",
    "        \n",
    "        print(f\"🚀 Submitting video request to Seedance 1.0 Pro...\")\n",
    "        result = subscribe(\n",
    "            \"fal-ai/bytedance/seedance/v1/pro/text-to-video\",\n",
    "            arguments={\n",
    "                \"prompt\": video_prompt,\n",
    "                \"aspect_ratio\": \"16:9\",\n",
    "                \"resolution\": \"1080p\",\n",
    "                \"duration\": \"10\",\n",
    "                \"seed\": 42,\n",
    "            },\n",
    "            with_logs=True,\n",
    "            on_queue_update=on_queue_update,\n",
    "        )\n",
    "        \n",
    "        video_url = result.get(\"video\", {}).get(\"url\")\n",
    "        if not video_url:\n",
    "            return f\"❌ No video URL returned.\"\n",
    "        \n",
    "        response = requests.get(video_url)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        video_path = os.path.join(os.getcwd(), f\"generated_video_{timestamp}.mp4\")\n",
    "        \n",
    "        with open(video_path, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "        \n",
    "        try:\n",
    "            display(Video(video_path, width=600, embed=True))\n",
    "        except Exception:\n",
    "            print(f\"⚠️ Could not display video in notebook.\")\n",
    "        \n",
    "        # Create unique key based on timestamp\n",
    "        unique_key = f\"video_path_{timestamp}\"\n",
    "        tool_context.state[unique_key] = video_path\n",
    "        \n",
    "        # Also maintain a list of all video paths for easy access\n",
    "        if \"all_video_paths\" not in tool_context.state:\n",
    "            tool_context.state[\"all_video_paths\"] = []\n",
    "        tool_context.state[\"all_video_paths\"].append(video_path)\n",
    "        \n",
    "        print(f\"✅ Successfully generated and saved video from Seedance 1.0 Pro\")\n",
    "        print(f\"📁 Video saved to: {video_path}\")\n",
    "        print(f\"🔑 Context key: {unique_key}\")\n",
    "        \n",
    "        return f\"✅ Successfully generated and saved video from Seedance 1.0 Pro. Video saved to: {video_path} (key: {unique_key})\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"❌ Error during Seedance 1.0 Pro video generation: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "931d74b8-1cd2-4002-99d5-bd932a481afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_video_kling2_1_master(tool_context: ToolContext, image_path: str, video_prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates 1 video using Kling 2.1 based on the provided image path. Video length is 5 seconds!\n",
    "    Stores the resulting video path in the tool_context.\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): Path to the image file to convert to video\n",
    "        video_prompt (str): Text prompt for video generation\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not image_path or not os.path.isfile(image_path):\n",
    "            return f\"❌ Invalid image_path provided: {image_path}\"\n",
    "\n",
    "        if video_prompt:\n",
    "            print(video_prompt)\n",
    "        \n",
    "        print(f\"🎥 Uploading image to FAL...\")\n",
    "        uploaded_url = upload_file(image_path)\n",
    "        \n",
    "        def on_queue_update(update):\n",
    "            if isinstance(update, InProgress):\n",
    "                for log in update.logs:\n",
    "                    print(log[\"message\"])\n",
    "        \n",
    "        print(f\"🚀 Submitting video request to Kling 2.1...\")\n",
    "        result = subscribe(\n",
    "            \"fal-ai/kling-video/v2.1/master/image-to-video\",\n",
    "            arguments={\n",
    "                \"prompt\": video_prompt,\n",
    "                \"image_url\": uploaded_url,\n",
    "                \"duration\": \"10\",\n",
    "                \"aspect_ratio\": \"16:9\",\n",
    "                \"negative_prompt\": \"blurry, distorted, unrealistic hands, warped faces, glitchy movement, pixelated, low quality, jerky camera, unmentioned characters, unnatural lighting\",\n",
    "                \"cfg_scale\": 0.9,\n",
    "            },\n",
    "            with_logs=True,\n",
    "            on_queue_update=on_queue_update,\n",
    "        )\n",
    "        \n",
    "        video_url = result.get(\"video\", {}).get(\"url\")\n",
    "        if not video_url:\n",
    "            return f\"❌ No video URL returned.\"\n",
    "        \n",
    "        response = requests.get(video_url)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "        video_path = os.path.join(os.getcwd(), f\"generated_video_{timestamp}.mp4\")\n",
    "        \n",
    "        with open(video_path, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "\n",
    "        try:\n",
    "            display(Video(video_path, width=600, embed=True))\n",
    "        except Exception:\n",
    "            print(f\"⚠️ Could not display video in notebook.\")\n",
    "\n",
    "        # Create unique key based on timestamp\n",
    "        unique_key = f\"video_path_{timestamp}\"\n",
    "        tool_context.state[unique_key] = video_path\n",
    "        \n",
    "        # Also maintain a list of all video paths for easy access\n",
    "        if \"all_video_paths\" not in tool_context.state:\n",
    "            tool_context.state[\"all_video_paths\"] = []\n",
    "        tool_context.state[\"all_video_paths\"].append(video_path)\n",
    "        \n",
    "        print(f\"✅ Successfully generated and saved video from Kling 2.1 Master\") \n",
    "        print(f\"📁 Video saved to: {video_path}\")\n",
    "        print(f\"🔑 Context key: {unique_key}\")\n",
    "        \n",
    "        return f\"✅ Successfully generated and saved video from Kling 2.1 Master. Video saved to: {video_path} (key: {unique_key})\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"❌ Error during Kling 2.1 video generation: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8796a52d-2002-4e05-82b9-a6e70f3330ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_video_seedance_pro(tool_context: ToolContext, image_path: str, video_prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates 1 video using Seedance 1.0 Pro based on the provided image path. Video length is 5 seconds!\n",
    "    Stores the resulting video path in the tool_context.\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): Path to the image file to convert to video\n",
    "        video_prompt (str): Text prompt for video generation\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not image_path or not os.path.isfile(image_path):\n",
    "            return f\"❌ Invalid image_path provided: {image_path}\"\n",
    "        if video_prompt:\n",
    "            print(video_prompt)\n",
    "        \n",
    "        print(f\"🎥 Uploading image to FAL...\")\n",
    "        uploaded_url = upload_file(image_path)\n",
    "        \n",
    "        def on_queue_update(update):\n",
    "            if isinstance(update, InProgress):\n",
    "                for log in update.logs:\n",
    "                    print(log[\"message\"])\n",
    "        \n",
    "        print(f\"🚀 Submitting video request to Seedance 1.0 Pro...\")\n",
    "        result = subscribe(\n",
    "            \"fal-ai/bytedance/seedance/v1/pro/image-to-video\",\n",
    "            arguments={\n",
    "                \"prompt\": video_prompt,\n",
    "                \"image_url\": uploaded_url,\n",
    "                \"duration\": \"10\",\n",
    "                \"resolution\": \"1080p\",\n",
    "                \"seed\": 42,\n",
    "            },\n",
    "            with_logs=True,\n",
    "            on_queue_update=on_queue_update,\n",
    "        )\n",
    "        \n",
    "        video_url = result.get(\"video\", {}).get(\"url\")\n",
    "        if not video_url:\n",
    "            return f\"❌ No video URL returned.\"\n",
    "        \n",
    "        response = requests.get(video_url)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "        video_path = os.path.join(os.getcwd(), f\"generated_video_{timestamp}.mp4\")\n",
    "\n",
    "        with open(video_path, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "        \n",
    "        try:\n",
    "            display(Video(video_path, width=600, embed=True))\n",
    "        except Exception:\n",
    "            print(f\"⚠️ Could not display video in notebook.\")\n",
    "\n",
    "        # Create unique key based on timestamp\n",
    "        unique_key = f\"video_path_{timestamp}\"\n",
    "        tool_context.state[unique_key] = video_path\n",
    "        \n",
    "        # Also maintain a list of all video paths for easy access\n",
    "        if \"all_video_paths\" not in tool_context.state:\n",
    "            tool_context.state[\"all_video_paths\"] = []\n",
    "        tool_context.state[\"all_video_paths\"].append(video_path)\n",
    "        \n",
    "        print(f\"✅ Successfully generated and saved video from Seedance 1.0 Pro.\")\n",
    "        print(f\"📁 Video saved to: {video_path}\")\n",
    "        print(f\"🔑 Context key: {unique_key}\")\n",
    "        \n",
    "        return f\"✅ Successfully generated and saved video from Seedance 1.0 Pro. Video saved to: {video_path} (key: {unique_key})\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"❌ Error during Seedance 1.0 Pro video generation: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4b4c5e39-8e9d-4134-a969-59f95e4f26f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_resolution(video_path):\n",
    "    \"\"\"Get video resolution using ffprobe\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run([\n",
    "            \"ffprobe\", \"-v\", \"quiet\", \"-print_format\", \"json\", \n",
    "            \"-show_streams\", \"-select_streams\", \"v:0\", video_path\n",
    "        ], capture_output=True, text=True, check=True)\n",
    "        \n",
    "        data = json.loads(result.stdout)\n",
    "        stream = data['streams'][0]\n",
    "        width = int(stream['width'])\n",
    "        height = int(stream['height'])\n",
    "        return width, height\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error getting resolution for {video_path}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def concatenate_videos(tool_context: ToolContext) -> str:\n",
    "    \"\"\"\n",
    "    Smart concatenation that detects resolutions and scales to match the first video.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get video paths from the all_video_paths list\n",
    "    all_video_paths = tool_context.state.get(\"all_video_paths\", [])\n",
    "    \n",
    "    if not all_video_paths:\n",
    "        return \"❌ No video paths found in tool context.\"\n",
    "    \n",
    "    if len(all_video_paths) < 2:\n",
    "        return f\"❌ Only {len(all_video_paths)} video found. Need at least 2 videos to concatenate.\"\n",
    "    \n",
    "    # Use the paths in the order they were added (chronological)\n",
    "    video_paths = all_video_paths\n",
    "    \n",
    "    for path in video_paths:\n",
    "        if not Path(path).is_file():\n",
    "            return f\"❌ File not found: {path}\"\n",
    "    \n",
    "    try:\n",
    "        # Get resolutions of all videos\n",
    "        print(\"🔍 Detecting video resolutions...\")\n",
    "        resolutions = []\n",
    "        for i, path in enumerate(video_paths):\n",
    "            w, h = get_video_resolution(path)\n",
    "            if w is None or h is None:\n",
    "                return f\"❌ Failed to detect video resolutions\"\n",
    "            resolutions.append((w, h))\n",
    "        \n",
    "        # Use first video's resolution as target\n",
    "        target_width, target_height = resolutions[0]\n",
    "        print(f\"Target resolution: {target_width}x{target_height}\")\n",
    "        \n",
    "        # Check if all videos have same resolution\n",
    "        all_same_resolution = all(res == resolutions[0] for res in resolutions)\n",
    "        \n",
    "        if all_same_resolution:\n",
    "            print(\"✅ All videos have same resolution - simple concat\")\n",
    "            video_inputs = \"\".join(f\"[{i}:v]\" for i in range(len(video_paths)))\n",
    "            filter_complex = f\"{video_inputs}concat=n={len(video_paths)}:v=1:a=0[outv]\"\n",
    "        else:\n",
    "            print(\"🔄 Different resolutions detected - scaling to match first video\")\n",
    "            # Scale all videos to first video's resolution with padding\n",
    "            scale_filters = []\n",
    "            concat_inputs = []\n",
    "            \n",
    "            for i in range(len(video_paths)):\n",
    "                scale_filter = (\n",
    "                    f\"[{i}:v]scale={target_width}:{target_height}:force_original_aspect_ratio=decrease,\"\n",
    "                    f\"pad={target_width}:{target_height}:(ow-iw)/2:(oh-ih)/2[v{i}]\"\n",
    "                )\n",
    "                scale_filters.append(scale_filter)\n",
    "                concat_inputs.append(f\"[v{i}]\")\n",
    "            \n",
    "            filter_complex = \";\".join(scale_filters) + \";\" + \"\".join(concat_inputs) + f\"concat=n={len(video_paths)}:v=1:a=0[outv]\"\n",
    "        \n",
    "        # Run ffmpeg with dynamic filter\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        output_path = os.path.join(os.getcwd(), f\"concatenated_video_{timestamp}.mp4\")\n",
    "        \n",
    "        # Build ffmpeg command\n",
    "        ffmpeg_cmd = [\"ffmpeg\"]\n",
    "        for path in video_paths:\n",
    "            ffmpeg_cmd.extend([\"-i\", path])\n",
    "        \n",
    "        ffmpeg_cmd.extend([\n",
    "            \"-filter_complex\", filter_complex,\n",
    "            \"-map\", \"[outv]\",\n",
    "            \"-preset\", \"veryfast\",\n",
    "            \"-y\", output_path\n",
    "        ])\n",
    "        \n",
    "        print(\"🎬 Running ffmpeg concatenation...\")\n",
    "        subprocess.run(ffmpeg_cmd, check=True)\n",
    "        \n",
    "        if not Path(output_path).is_file():\n",
    "            return \"❌ Failed to create concatenated video.\"\n",
    "        \n",
    "        tool_context.state[\"concatenated_video_path\"] = output_path\n",
    "        return f\"✅ Successfully concatenated video at {output_path} (resolution: {target_width}x{target_height})\"\n",
    "        \n",
    "    except subprocess.CalledProcessError as e:\n",
    "        return f\"❌ FFmpeg error: {e}\"\n",
    "    except Exception as e:\n",
    "        return f\"❌ Unexpected error: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "78d1f2ac-e1f8-489d-bd63-9e1d33db778d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_audio_lyria(tool_context: ToolContext, audio_prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates 30s of music from the combined video prompts using Lyria 2.\n",
    "    Saves the audio to `lyria_output.wav` and updates `audio_path` in the tool_context.\n",
    "\n",
    "    Args:\n",
    "        audio_prompt (str): Text prompt for audio generation by Lyria.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\n🎼 Generating music with Lyria 2...\\nPrompt:\\n\", audio_prompt)\n",
    "\n",
    "    try:\n",
    "        def on_queue_update(update):\n",
    "            if isinstance(update, InProgress):\n",
    "                for log in update.logs:\n",
    "                    print(log[\"message\"])\n",
    "\n",
    "        result = subscribe(\n",
    "            \"fal-ai/lyria2\",\n",
    "            arguments={\n",
    "                \"prompt\": audio_prompt,\n",
    "                \"negative_prompt\": \"vocals, dissonance, low quality, harsh noise\"\n",
    "            },\n",
    "            with_logs=True,\n",
    "            on_queue_update=on_queue_update,\n",
    "        )\n",
    "\n",
    "        audio_url = result.get(\"audio\", {}).get(\"url\")\n",
    "        if not audio_url:\n",
    "            return \"❌ Failed to retrieve audio URL from Lyria.\"\n",
    "\n",
    "        response = requests.get(audio_url)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        audio_path = os.path.join(os.getcwd(), \"lyria_output.wav\")\n",
    "        with open(audio_path, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "\n",
    "        try:\n",
    "            IPAudio(audio_path)\n",
    "        except Exception:\n",
    "            print(\"⚠️ Could not display audio in notebook.\")\n",
    "\n",
    "        tool_context.state[\"audio_prompt\"] = audio_prompt\n",
    "        tool_context.state[\"audio_path\"] = audio_path\n",
    "\n",
    "        return f\"✅ Music successfully generated and saved at {audio_path}\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"❌ Error generating music with Lyria 2: {e}\"\n",
    "\n",
    "\n",
    "def merge_video_and_audio(_: str, tool_context: ToolContext) -> str:\n",
    "    \"\"\"\n",
    "    Uses FFmpeg to layer 30s Lyria music over a 30s final video.\n",
    "    Saves to `final_video_with_audio.mp4` and updates `tool_context.state`.\n",
    "    \"\"\"\n",
    "\n",
    "    video_path = tool_context.state.get(\"concatenated_video_path\")\n",
    "    audio_path = tool_context.state.get(\"audio_path\")\n",
    "\n",
    "    if not video_path or not audio_path:\n",
    "        return \"❌ Missing video or audio path.\"\n",
    "\n",
    "    if not Path(video_path).is_file():\n",
    "        return f\"❌ Video not found: {video_path}\"\n",
    "    if not Path(audio_path).is_file():\n",
    "        return f\"❌ Audio not found: {audio_path}\"\n",
    "\n",
    "    output_path = os.path.join(os.getcwd(), \"final_video_with_audio.mp4\")\n",
    "\n",
    "    try:\n",
    "        subprocess.run([\n",
    "            \"ffmpeg\",\n",
    "            \"-i\", video_path,\n",
    "            \"-i\", audio_path,\n",
    "            \"-shortest\",\n",
    "            \"-c:v\", \"copy\",\n",
    "            \"-c:a\", \"aac\",\n",
    "            \"-b:a\", \"192k\",\n",
    "            \"-y\", output_path\n",
    "        ], check=True)\n",
    "\n",
    "        if not Path(output_path).is_file():\n",
    "            return \"❌ Failed to create final video with audio.\"\n",
    "\n",
    "        tool_context.state[\"final_video_with_audio\"] = output_path\n",
    "        return f\"✅ Final video with audio is created and saved at {output_path}\"\n",
    "\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        return f\"❌ FFmpeg error: {e}\"\n",
    "    except Exception as e:\n",
    "        return f\"❌ Unexpected error: {e}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dbafa750-b3b7-45fe-8126-6cda941adb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "like_no_other_agent = ADKAgent(\n",
    "    name=\"like_no_other_agent\",\n",
    "    model=\"gemini-2.5-pro\", #gemini-2.5-flash-preview-05-20, gemini-2.0-flash, gemini-2.5-pro\n",
    "    description=\"Recreates your favourite ad in an unexpected way\",\n",
    "    instruction=\"\"\"\n",
    "        **Persona:** You are a world-class creative director and brand strategist, known for your innovative, daring, and slightly rebellious approach to advertising. Your goal is to deconstruct a famous ad campaign and rebuild it with the tools you havem creating a stunning tribute to the original piece of art - ad campaign.\n",
    "\n",
    "        **Core Mission:** Given a user's request about a famous advertisement, you will analyze its core message, style, and cultural impact. \n",
    "        You will then try to recreate the ad based on the information you've gathered, delivered as a short video with matching audio.\n",
    "\n",
    "\n",
    "        **Workflow:**\n",
    "\n",
    "        1.  **Deconstruct the Original:**\n",
    "            * Use the `ask_openai_agent` tool to get a comprehensive description of the original ad campaign mentioned by the user.\n",
    "            * Analyze this information to pinpoint the absolute core message, the scenes, and the emotional response it was designed to evoke.\n",
    "\n",
    "        2.  **Create a Storyboard and Shot List:**\n",
    "            * Create a plan for the video. This will be a sequence of scenes. Decide if you need to generate still images first and then animate them, or if you can generate video clips directly.\n",
    "            * It is truly important to create one cohesive video and not just random scenes.\n",
    "\n",
    "        3. You have the following tools available to assemble the final video:\n",
    "            * `concatenate_videos` - Use this if more than one video has been generated and needs to be stitched together.\n",
    "            * `text_to_audio_lyria` - Generates a matching audio track based on the ad research. **Do not mention artist names to avoid content policy violations!**\n",
    "            * `merge_video_and_audio` - Combines the selected video and audio into the final output.\n",
    "\n",
    "        **Masterful Prompt Engineering for Visuals:**\n",
    "            * For each visual asset (image or video), you will craft an expert-level prompt. **Do not** use simple prompts. Use very elaborate and precise prompts instead. Your prompts must include details on:\n",
    "                * **Subject & Composition:** What is the main focus? Use terms like \"close-up,\" \"medium shot,\" \"wide shot,\" \"establishing shot.\"\n",
    "                * **Action & Emotion:** What are the characters (or objects) doing and feeling? Be vivid, be thorough, be explicit!\n",
    "                * **Cinematography:** Specify camera angles (\"low-angle shot,\" \"dutch angle\"), lighting (\"dramatic lighting,\" \"soft morning light,\" \"neon glow\"), and lens effects (\"shallow depth of field,\" \"lens flare\").\n",
    "                * **Art Style:** Be highly descriptive. Examples: \"Photorealistic, cinematic, 8k,\" \"1990s grainy VHS style,\" \"Japanese anime aesthetic, Studio Ghibli inspired,\" \"surrealism, Salvador Dalí influence.\"\n",
    "                * **Example Prompt:** *Instead of \"a man drinking a soda,\" write: \"Extreme close-up on a man's face, cinematic lighting, he's sweating after a long run. He opens a can of soda, a look of intense satisfaction on his face. The shot is photorealistic, with a shallow depth of field, capturing the condensation on the can. 8k.\"*\n",
    "\n",
    "\n",
    "        **IMPORTANT:** Tools use\n",
    "            * Use tools randomly — do not show preference for one model over another.\n",
    "                Once you’ve selected your image/video tools during planning, do not change them during the video production process.\n",
    "\n",
    "            * CRITICAL to follow: If you choose to first do text-to-image and then image-to-video, you MUST generate the image immediately before generating the video.\n",
    "                The image is NOT stored in the context — every new image generation will overwrite the previous one.\n",
    "                Only videos are stored in tool_context, so you **must not** generate multiple images first and then animate them later using a video tool.\n",
    "\n",
    "            * Image tools:\n",
    "                * text_to_image_imagen4\n",
    "                * text_to_image_gpt_image_1\n",
    "                * text_to_image_flux_pro\n",
    "            \n",
    "            * Video tools specification:\n",
    "                * `text_to_video_veo3` - producses 8s videos & the \"max prompt size\" for Veo3 is dictated not by a character count but by the 8-second runtime of the output.\n",
    "                * `image_to_video_kling2_1_master` tool produces 10s videos; maximum prompt size is 2500 characters \n",
    "                * `image_to_video_seedance_pro` tool produces 10s videos & maximum prompt size is 500 characters\n",
    "                * `concatenate_videos` tool enables you to concatenate videos to achieve longer video lenght than the one supported by each video model/tool.\n",
    "                \n",
    "        \n",
    "        **IMPORTANT CONSTRAINTS:**\n",
    "            * You must always begin by using `ask_openai_agent` to understand the original ad.\n",
    "            * You must use step-by-step thinking to outline your campaing recreation plan before calling any creative tools. You must try to create the campaign as close as possible to the original based on the information you've gathered!\n",
    "            * When duration is not provided by the user, video **MUST NOT** exceed 30 seconds, otherwise observe user provided duration. You are allowed to plan the duration if not provided by the user and generate e.g. 24s videos or other video duration under 30 seconds!\n",
    "            * The final video should be a cohesive narrative, not just a collection of random clips.\n",
    "            * You must always produce the final video with audio.\n",
    "        \"\"\",\n",
    "    tools=[\n",
    "        ask_openai_agent,\n",
    "        read_markdown,\n",
    "        text_to_image_imagen4,\n",
    "        text_to_image_gpt_image_1,\n",
    "        text_to_image_flux_pro,\n",
    "        # text_to_video_seedance_pro,\n",
    "        text_to_video_veo3,\n",
    "        image_to_video_seedance_pro, \n",
    "        image_to_video_kling2_1_master,\n",
    "        concatenate_videos,\n",
    "        text_to_audio_lyria,\n",
    "        merge_video_and_audio,\n",
    "    ],\n",
    "    planner=BuiltInPlanner(\n",
    "          thinking_config=types.ThinkingConfig(\n",
    "              include_thoughts=True,      # capture intermediate reasoning\n",
    "              thinking_budget=2048        # tokens allocated for planning 2048, 3072\n",
    "          )\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71e41fc-0c6e-4a56-b21d-b7b9d0bb169c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "async def main():\n",
    "    session_service = InMemorySessionService()\n",
    "    runner = ADKRunner(agent=like_no_other_agent, app_name=\"ad_app\", session_service=session_service)\n",
    "    \n",
    "    # user_prompt = \"\"\"Tell me what you know about the color like no other ad campaign done for sony bravia. It involves bouncing balls released on the streets of San Francisco.\"\"\"\n",
    "    # user_prompt = \"\"\"Tell me what you know about the epic split ad campaign done for Volvo by Jean-Claude Van Damme. I want 30s video.\"\"\"\n",
    "    # user_prompt = \"\"\"This is a test, generate just one image, doesn't mater what, using the text_to_image_imagen4 tool\"\"\"\n",
    "    # user_prompt = \"\"\"Recreate the Argentinian Coca‑Cola Parents commercial also called Coca Cola Family. \n",
    "    #  It is about about young family getting a kid. You must use these tools: text_to_image_gpt_image_1 and image_to_video_seedance_pro. \n",
    "    #  I want 30s video\"\"\"\n",
    "    user_prompt = \"\"\"Recreate Lindsey Vonn - wait for it it ad for Head ski.  You must use these tools: text_to_image_gpt_image_1 and image_to_video_kling2_1_master. \n",
    "     I want 30s video\"\"\"\n",
    "    \n",
    "\n",
    "    \n",
    "    print(f\"🚀 Starting campaign with query: {user_prompt[:50]}...\")\n",
    "    \n",
    "    user_message = types.Content(\n",
    "        role=\"user\",\n",
    "        parts=[types.Part(text=user_prompt)]\n",
    "    )\n",
    "    \n",
    "    session = await session_service.create_session(\n",
    "        app_name=\"ad_app\", \n",
    "        user_id=\"jeny\", \n",
    "        session_id=\"session_001\",\n",
    "        state={\n",
    "            \"user_query\": user_prompt\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    print(f\"✅ Session created with state keys: {list(session.state.keys())}\")\n",
    "    \n",
    "    events = []\n",
    "    \n",
    "    # Run the agent with detailed logging\n",
    "    async for event in runner.run_async(user_id=\"jeny\", session_id=session.id, new_message=user_message):\n",
    "        events.append(event)\n",
    "        print(\"\\n────────── RAW EVENT ──────────\")\n",
    "        print(json.dumps(event.model_dump(), indent=2, default=str))\n",
    "        print(\n",
    "            f\"[{type(event).__name__:<25}] \"\n",
    "            f\"author={event.author:<15} \"\n",
    "            f\"final={event.is_final_response()}\"\n",
    "        )\n",
    "        # Print content parts (LLM output)\n",
    "        if event.content and event.content.parts:\n",
    "            for part in event.content.parts:\n",
    "                if part.text:\n",
    "                    print(\"💬 text →\", part.text.strip())\n",
    "        # Tool calls\n",
    "        for call in event.get_function_calls():\n",
    "            print(f\"🔧 tool‑call → {call.name}({json.dumps(call.args, indent=2)})\")\n",
    "        # Tool results\n",
    "        for resp in event.get_function_responses():\n",
    "            print(f\"📤 tool‑result → {resp.name} → {json.dumps(resp.response, indent=2)}\")\n",
    "        # State delta\n",
    "        if event.actions and event.actions.state_delta:\n",
    "            print(\"🧠 state Δ →\", json.dumps(event.actions.state_delta, indent=2))\n",
    "        if event.actions and event.actions.artifact_delta:\n",
    "            print(\"📦 artifact Δ →\", json.dumps(event.actions.artifact_delta, indent=2))\n",
    "    \n",
    "    # Final output\n",
    "    final_text = (\n",
    "        events[-1].content.parts[0].text\n",
    "        if events and events[-1].content and events[-1].content.parts else \"\"\n",
    "    )\n",
    "    print(\"\\n===== ✅ FINAL ANSWER =====\")\n",
    "    print(final_text or \"(no text)\")\n",
    "\n",
    "# Execute the main function\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1640fe05-c636-4a76-b295-cea0c6b5d1d9",
   "metadata": {},
   "source": [
    "> **Note: Run the cell below if you don't want to get the full agent output**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568fa21f-8981-4d59-a19e-243857907b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# async def main():\n",
    "#     session_service = InMemorySessionService()\n",
    "#     runner = ADKRunner(agent=like_no_other_agent, app_name=\"ad_app\", session_service=session_service)\n",
    "    \n",
    "#     user_prompt = \"\"\"Tell me what you know about the like no other ad campaign done for sony bravia. there were bouncing balls let on the streets of San Francisco. I want 8s video.\"\"\"\n",
    "    \n",
    "#     print(f\"🚀 Starting campaign with query: {user_prompt[:50]}...\")\n",
    "    \n",
    "#     user_message = types.Content(\n",
    "#         role=\"user\",\n",
    "#         parts=[types.Part(text=user_prompt)]\n",
    "#     )\n",
    "    \n",
    "#     session = await session_service.create_session(\n",
    "#         app_name=\"ad_app\", \n",
    "#         user_id=\"jeny\", \n",
    "#         session_id=\"session_001\",\n",
    "#         state={\n",
    "#             \"user_query\": user_prompt\n",
    "#         }\n",
    "#     )\n",
    "    \n",
    "#     print(f\"✅ Session created with state keys: {list(session.state.keys())}\")\n",
    "    \n",
    "#     # Run the agent\n",
    "#     async for event in runner.run_async(user_id=\"jeny\", session_id=session.id, new_message=user_message):\n",
    "#         if event.is_final_response():\n",
    "#             if event.content and event.content.parts:\n",
    "#                 print(event.content.parts[0].text)\n",
    "#             else:\n",
    "#                 print(\"No final text response was returned by the agent.\")\n",
    "\n",
    "# # Execute the main function\n",
    "# await main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.13 (venv)",
   "language": "python",
   "name": "jupyter-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
